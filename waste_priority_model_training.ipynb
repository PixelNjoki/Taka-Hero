{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taka-Hero: Waste Report Priority Classification Model\n",
    "\n",
    "## Overview\n",
    "This notebook builds a multi-modal AI system that:\n",
    "1. Classifies waste images (hazardous, medical, plastic, organic, etc.)\n",
    "2. Analyzes text descriptions for urgency indicators\n",
    "3. Assigns priority levels: **Critical (Red)**, **Urgent (Orange)**, **Important (Black)**, **Secondary (Blue)**\n",
    "4. Generates solution suggestions\n",
    "\n",
    "## Priority Classification Logic\n",
    "- **Critical (Red)**: Hazardous waste, medical waste, toxic materials, immediate health risks\n",
    "- **Urgent (Orange)**: Large illegal dumps, blocked waterways, accumulating waste in public areas\n",
    "- **Important (Black)**: Public space littering, recyclable accumulation, infrastructure concerns\n",
    "- **Secondary (Blue)**: Minor littering, maintenance issues, single-item reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install transformers datasets pillow scikit-learn matplotlib seaborn\n",
    "!pip install timm kaggle opencv-python"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We'll use publicly available waste classification datasets:\n",
    "- **TrashNet**: Waste classification dataset\n",
    "- **Waste Classification Data**: From Kaggle\n",
    "- **Synthetic text data**: Created based on waste reporting patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create directories\n",
    "os.makedirs('data/waste_images', exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data/training', exist_ok=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download dataset (using Kaggle API - requires kaggle.json in ~/.kaggle/)\n",
    "# Alternative: Manual download from https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\n",
    "\n",
    "try:\n",
    "    !kaggle datasets download -d asdasdasasdas/garbage-classification -p data/waste_images --unzip\n",
    "    print(\"Dataset downloaded successfully!\")\n",
    "except:\n",
    "    print(\"Kaggle API not configured. Please download dataset manually from:\")\n",
    "    print(\"https://www.kaggle.com/datasets/asdasdasasdas/garbage-classification\")\n",
    "    print(\"Extract to: data/waste_images/\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Synthetic Training Data with Priority Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define waste categories and their base priority levels\n",
    "waste_categories = {\n",
    "    'medical': {'priority': 'Critical', 'risk_score': 10},\n",
    "    'hazardous': {'priority': 'Critical', 'risk_score': 10},\n",
    "    'chemical': {'priority': 'Critical', 'risk_score': 9},\n",
    "    'electronic': {'priority': 'Urgent', 'risk_score': 7},\n",
    "    'plastic': {'priority': 'Important', 'risk_score': 5},\n",
    "    'metal': {'priority': 'Important', 'risk_score': 5},\n",
    "    'glass': {'priority': 'Important', 'risk_score': 6},\n",
    "    'paper': {'priority': 'Secondary', 'risk_score': 3},\n",
    "    'cardboard': {'priority': 'Secondary', 'risk_score': 2},\n",
    "    'organic': {'priority': 'Secondary', 'risk_score': 4},\n",
    "}\n",
    "\n",
    "# Urgency keywords that modify priority\n",
    "urgency_modifiers = {\n",
    "    'critical_keywords': ['leaking', 'toxic', 'hospital', 'children', 'school', 'poisonous', \n",
    "                          'dangerous', 'medical', 'needles', 'syringes', 'chemical', 'burning'],\n",
    "    'urgent_keywords': ['large', 'blocking', 'waterway', 'drain', 'river', 'accumulating', \n",
    "                        'illegal dump', 'tons', 'truck', 'growing', 'weeks'],\n",
    "    'important_keywords': ['public', 'street', 'road', 'park', 'market', 'many', 'scattered'],\n",
    "    'secondary_keywords': ['small', 'single', 'item', 'few', 'minor']\n",
    "}\n",
    "\n",
    "# Generate synthetic text descriptions\n",
    "critical_descriptions = [\n",
    "    \"Medical waste including syringes found near school playground\",\n",
    "    \"Leaking chemical containers in residential area\",\n",
    "    \"Hospital waste dumped in public space with children playing nearby\",\n",
    "    \"Toxic materials discovered near water source\",\n",
    "    \"Hazardous waste leaking into drain system\",\n",
    "    \"Medical needles and bandages scattered in park\",\n",
    "    \"Chemical drums leaking dangerous substances\",\n",
    "    \"Burning toxic waste creating harmful smoke\"\n",
    "]\n",
    "\n",
    "urgent_descriptions = [\n",
    "    \"Large illegal dump blocking main road\",\n",
    "    \"Tons of waste blocking drainage system causing flooding risk\",\n",
    "    \"Massive accumulation of waste in river blocking water flow\",\n",
    "    \"Several truckloads of waste dumped in public area\",\n",
    "    \"Growing pile of electronic waste in market area\",\n",
    "    \"Large amount of plastic waste blocking waterway\",\n",
    "    \"Illegal dumping site expanding for weeks\"\n",
    "]\n",
    "\n",
    "important_descriptions = [\n",
    "    \"Scattered plastic bottles and containers in public park\",\n",
    "    \"Mixed recyclable waste accumulating on street corner\",\n",
    "    \"Multiple bags of waste left on roadside\",\n",
    "    \"Glass bottles and metal cans scattered in market\",\n",
    "    \"Paper and cardboard waste piling up in public area\",\n",
    "    \"General waste littering along the street\"\n",
    "]\n",
    "\n",
    "secondary_descriptions = [\n",
    "    \"Single plastic bag on sidewalk\",\n",
    "    \"Few cardboard boxes near bus stop\",\n",
    "    \"Small amount of paper waste in corner\",\n",
    "    \"Minor organic waste from fruit vendor\",\n",
    "    \"A few items of litter on pathway\",\n",
    "    \"Single bottle left in public space\"\n",
    "]\n",
    "\n",
    "print(f\"Generated {len(critical_descriptions)} critical descriptions\")\n",
    "print(f\"Generated {len(urgent_descriptions)} urgent descriptions\")\n",
    "print(f\"Generated {len(important_descriptions)} important descriptions\")\n",
    "print(f\"Generated {len(secondary_descriptions)} secondary descriptions\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Image Classification Model (Waste Type Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WasteImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.image_paths[idx]}: {e}\")\n",
    "            # Return a blank image if loading fails\n",
    "            return torch.zeros((3, 224, 224)), self.labels[idx]\n",
    "\n",
    "# Image transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WasteImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(WasteImageClassifier, self).__init__()\n",
    "        # Use pretrained EfficientNet\n",
    "        self.backbone = models.efficientnet_b0(pretrained=True)\n",
    "        \n",
    "        # Freeze early layers\n",
    "        for param in list(self.backbone.parameters())[:-20]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace classifier\n",
    "        num_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Define waste type classes\n",
    "waste_classes = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'organic']\n",
    "print(f\"Waste classes: {waste_classes}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Text Classification Model (Urgency Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TextUrgencyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(TextUrgencyClassifier, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Freeze BERT layers except last 2\n",
    "        for param in list(self.bert.parameters())[:-12]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        return self.classifier(pooled_output)\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "print(\"Text model initialized\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Modal Priority Classifier (Combined Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class MultiModalPriorityClassifier(nn.Module):\n",
    "    def __init__(self, num_priority_classes=4):\n",
    "        super(MultiModalPriorityClassifier, self).__init__()\n",
    "        \n",
    "        # Image feature extractor\n",
    "        self.image_model = models.efficientnet_b0(pretrained=True)\n",
    "        num_image_features = self.image_model.classifier[1].in_features\n",
    "        self.image_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Text feature extractor\n",
    "        self.text_model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Freeze most layers\n",
    "        for param in list(self.image_model.parameters())[:-10]:\n",
    "            param.requires_grad = False\n",
    "        for param in list(self.text_model.parameters())[:-6]:\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion layer\n",
    "        combined_features = num_image_features + 768  # EfficientNet + DistilBERT\n",
    "        \n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(combined_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_priority_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, input_ids, attention_mask):\n",
    "        # Extract image features\n",
    "        image_features = self.image_model(image)\n",
    "        \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined = torch.cat([image_features, text_features], dim=1)\n",
    "        \n",
    "        # Classify priority\n",
    "        priority_logits = self.fusion(combined)\n",
    "        \n",
    "        return priority_logits\n",
    "\n",
    "# Priority classes\n",
    "priority_classes = ['Critical', 'Urgent', 'Important', 'Secondary']\n",
    "priority_colors = {'Critical': 'red', 'Urgent': 'orange', 'Important': 'black', 'Secondary': 'blue'}\n",
    "print(f\"Priority classes: {priority_classes}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solution Suggestion System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SolutionGenerator:\n",
    "    def __init__(self):\n",
    "        self.solutions = {\n",
    "            'Critical': {\n",
    "                'medical': [\n",
    "                    \"Immediately cordon off the area to prevent public access\",\n",
    "                    \"Contact specialized medical waste disposal team\",\n",
    "                    \"Alert local health authorities\",\n",
    "                    \"Deploy hazmat-trained personnel with proper PPE\",\n",
    "                    \"Arrange for biomedical waste incinerator disposal\"\n",
    "                ],\n",
    "                'hazardous': [\n",
    "                    \"Evacuate immediate area and establish safety perimeter\",\n",
    "                    \"Contact environmental protection agency\",\n",
    "                    \"Deploy hazardous materials response team\",\n",
    "                    \"Arrange containment and specialized disposal\",\n",
    "                    \"Monitor environmental impact\"\n",
    "                ],\n",
    "                'chemical': [\n",
    "                    \"Secure area and prevent access\",\n",
    "                    \"Contact chemical emergency response team\",\n",
    "                    \"Identify chemical type for proper handling\",\n",
    "                    \"Arrange specialized hazmat disposal\",\n",
    "                    \"Test surrounding soil and water\"\n",
    "                ]\n",
    "            },\n",
    "            'Urgent': {\n",
    "                'blocking': [\n",
    "                    \"Deploy cleanup crew within 24 hours\",\n",
    "                    \"Arrange heavy equipment for large waste removal\",\n",
    "                    \"Clear drainage systems to prevent flooding\",\n",
    "                    \"Set up temporary barriers to prevent further dumping\",\n",
    "                    \"Investigate illegal dumping source\"\n",
    "                ],\n",
    "                'accumulating': [\n",
    "                    \"Schedule immediate waste collection\",\n",
    "                    \"Deploy multiple collection vehicles\",\n",
    "                    \"Increase collection frequency for this area\",\n",
    "                    \"Add temporary collection points\"\n",
    "                ],\n",
    "                'electronic': [\n",
    "                    \"Arrange e-waste recycling specialist\",\n",
    "                    \"Safely extract valuable materials\",\n",
    "                    \"Dispose batteries and hazardous components properly\",\n",
    "                    \"Partner with certified e-waste recyclers\"\n",
    "                ]\n",
    "            },\n",
    "            'Important': {\n",
    "                'plastic': [\n",
    "                    \"Schedule waste collection within 3 days\",\n",
    "                    \"Sort and send to recycling facility\",\n",
    "                    \"Deploy standard cleanup crew\",\n",
    "                    \"Install additional bins in area\"\n",
    "                ],\n",
    "                'general': [\n",
    "                    \"Add to regular collection schedule\",\n",
    "                    \"Send municipal cleanup team\",\n",
    "                    \"Increase bin capacity in area\",\n",
    "                    \"Post waste disposal guidelines\"\n",
    "                ]\n",
    "            },\n",
    "            'Secondary': {\n",
    "                'minor': [\n",
    "                    \"Include in next scheduled collection round\",\n",
    "                    \"Send maintenance crew when available\",\n",
    "                    \"Monitor for accumulation\",\n",
    "                    \"Community volunteer cleanup possible\"\n",
    "                ],\n",
    "                'organic': [\n",
    "                    \"Collect during regular rounds\",\n",
    "                    \"Consider composting program\",\n",
    "                    \"Educate vendors on waste management\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def generate_solution(self, priority, waste_type, description):\n",
    "        \"\"\"Generate contextual solution suggestions\"\"\"\n",
    "        solutions = []\n",
    "        \n",
    "        # Get priority-level solutions\n",
    "        priority_solutions = self.solutions.get(priority, {})\n",
    "        \n",
    "        # Try to match specific waste type\n",
    "        if waste_type in priority_solutions:\n",
    "            solutions.extend(priority_solutions[waste_type])\n",
    "        else:\n",
    "            # Use general solutions for this priority\n",
    "            for key, sols in priority_solutions.items():\n",
    "                solutions.extend(sols[:2])  # Take first 2 from each category\n",
    "        \n",
    "        # Add context-specific suggestions based on description\n",
    "        description_lower = description.lower()\n",
    "        if 'water' in description_lower or 'river' in description_lower:\n",
    "            solutions.insert(0, \"Prevent water contamination - prioritize waterway clearing\")\n",
    "        if 'school' in description_lower or 'children' in description_lower:\n",
    "            solutions.insert(0, \"Child safety priority - expedite removal near educational facilities\")\n",
    "        if 'market' in description_lower:\n",
    "            solutions.append(\"Coordinate with market authorities for ongoing management\")\n",
    "        \n",
    "        return list(dict.fromkeys(solutions[:5]))  # Return unique top 5 suggestions\n",
    "\n",
    "solution_generator = SolutionGenerator()\n",
    "print(\"Solution generator initialized\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Function (Simplified for demonstration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_priority_model(model, train_loader, val_loader, num_epochs=10, lr=0.001):\n",
    "    \"\"\"Train the multi-modal priority classification model\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, input_ids, attention_mask, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images, input_ids, attention_mask)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "        \n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_acc'].append(val_accuracy)\n",
    "        \n",
    "        print(f'\\nEpoch [{epoch+1}/{num_epochs}]:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\\n')\n",
    "        \n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'models/best_priority_model.pth')\n",
    "            print(\"Best model saved!\")\n",
    "    \n",
    "    return history"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Synthetic Dataset for Training Demo\n",
    "\n",
    "**Note**: In production, you would use real waste images. For this demo, we'll create a synthetic dataset structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create synthetic training data structure\n",
    "training_data = []\n",
    "\n",
    "# Critical priority samples\n",
    "for desc in critical_descriptions:\n",
    "    training_data.append({\n",
    "        'description': desc,\n",
    "        'priority': 'Critical',\n",
    "        'priority_label': 0,\n",
    "        'waste_type': 'hazardous'\n",
    "    })\n",
    "\n",
    "# Urgent priority samples\n",
    "for desc in urgent_descriptions:\n",
    "    training_data.append({\n",
    "        'description': desc,\n",
    "        'priority': 'Urgent',\n",
    "        'priority_label': 1,\n",
    "        'waste_type': 'mixed'\n",
    "    })\n",
    "\n",
    "# Important priority samples\n",
    "for desc in important_descriptions:\n",
    "    training_data.append({\n",
    "        'description': desc,\n",
    "        'priority': 'Important',\n",
    "        'priority_label': 2,\n",
    "        'waste_type': 'recyclable'\n",
    "    })\n",
    "\n",
    "# Secondary priority samples\n",
    "for desc in secondary_descriptions:\n",
    "    training_data.append({\n",
    "        'description': desc,\n",
    "        'priority': 'Secondary',\n",
    "        'priority_label': 3,\n",
    "        'waste_type': 'general'\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(training_data)\n",
    "print(f\"\\nTraining dataset created with {len(df)} samples\")\n",
    "print(f\"\\nPriority distribution:\")\n",
    "print(df['priority'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Prediction Pipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class WastePriorityPredictor:\n",
    "    def __init__(self, model_path='models/best_priority_model.pth'):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = MultiModalPriorityClassifier(num_priority_classes=4)\n",
    "        \n",
    "        if os.path.exists(model_path):\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=self.device))\n",
    "            print(f\"Model loaded from {model_path}\")\n",
    "        else:\n",
    "            print(\"Warning: Model file not found. Using rule-based classification.\")\n",
    "        \n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.transform = val_transform\n",
    "        self.priority_classes = ['Critical', 'Urgent', 'Important', 'Secondary']\n",
    "        self.priority_colors = {'Critical': 'red', 'Urgent': 'orange', 'Important': 'black', 'Secondary': 'blue'}\n",
    "        self.solution_generator = SolutionGenerator()\n",
    "        \n",
    "        # Rule-based fallback keywords\n",
    "        self.critical_keywords = ['medical', 'toxic', 'hazardous', 'chemical', 'dangerous', \n",
    "                                  'poisonous', 'leaking', 'hospital', 'needles', 'syringes']\n",
    "        self.urgent_keywords = ['blocking', 'large', 'tons', 'truck', 'illegal dump', \n",
    "                               'drain', 'waterway', 'river', 'flooding']\n",
    "        self.important_keywords = ['street', 'road', 'public', 'park', 'market', 'scattered']\n",
    "    \n",
    "    def rule_based_priority(self, description, image_path=None):\n",
    "        \"\"\"Fallback rule-based priority classification\"\"\"\n",
    "        desc_lower = description.lower()\n",
    "        \n",
    "        # Check for critical keywords\n",
    "        if any(keyword in desc_lower for keyword in self.critical_keywords):\n",
    "            return 'Critical', 0.95\n",
    "        \n",
    "        # Check for urgent keywords\n",
    "        if any(keyword in desc_lower for keyword in self.urgent_keywords):\n",
    "            return 'Urgent', 0.85\n",
    "        \n",
    "        # Check for important keywords\n",
    "        if any(keyword in desc_lower for keyword in self.important_keywords):\n",
    "            return 'Important', 0.75\n",
    "        \n",
    "        # Default to secondary\n",
    "        return 'Secondary', 0.65\n",
    "    \n",
    "    def predict(self, image_path, description):\n",
    "        \"\"\"Predict priority for a waste report\"\"\"\n",
    "        try:\n",
    "            # Load and transform image\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "            \n",
    "            # Tokenize description\n",
    "            encoding = self.tokenizer(\n",
    "                description,\n",
    "                max_length=128,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids = encoding['input_ids'].to(self.device)\n",
    "            attention_mask = encoding['attention_mask'].to(self.device)\n",
    "            \n",
    "            # Model prediction\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(image_tensor, input_ids, attention_mask)\n",
    "                probabilities = torch.softmax(outputs, dim=1)\n",
    "                confidence, predicted_idx = torch.max(probabilities, 1)\n",
    "                \n",
    "                predicted_priority = self.priority_classes[predicted_idx.item()]\n",
    "                confidence_score = confidence.item()\n",
    "            \n",
    "            # If confidence is low, use rule-based approach\n",
    "            if confidence_score < 0.6:\n",
    "                predicted_priority, confidence_score = self.rule_based_priority(description, image_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Model prediction error: {e}. Using rule-based classification.\")\n",
    "            predicted_priority, confidence_score = self.rule_based_priority(description, image_path)\n",
    "        \n",
    "        # Detect waste type from description (simple keyword matching)\n",
    "        waste_type = self.detect_waste_type(description)\n",
    "        \n",
    "        # Generate solutions\n",
    "        solutions = self.solution_generator.generate_solution(\n",
    "            predicted_priority, waste_type, description\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'priority': predicted_priority,\n",
    "            'confidence': float(confidence_score),\n",
    "            'color': self.priority_colors[predicted_priority],\n",
    "            'waste_type': waste_type,\n",
    "            'solutions': solutions,\n",
    "            'status': 'Pending'\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def detect_waste_type(self, description):\n",
    "        \"\"\"Detect waste type from description using keywords\"\"\"\n",
    "        desc_lower = description.lower()\n",
    "        \n",
    "        waste_keywords = {\n",
    "            'medical': ['medical', 'hospital', 'syringe', 'needle', 'bandage'],\n",
    "            'hazardous': ['hazardous', 'toxic', 'chemical', 'dangerous', 'poisonous'],\n",
    "            'electronic': ['electronic', 'e-waste', 'computer', 'phone', 'battery'],\n",
    "            'plastic': ['plastic', 'bottle', 'bag', 'container'],\n",
    "            'organic': ['organic', 'food', 'fruit', 'vegetable'],\n",
    "            'metal': ['metal', 'can', 'aluminum', 'steel'],\n",
    "            'glass': ['glass', 'bottle'],\n",
    "            'paper': ['paper', 'cardboard'],\n",
    "        }\n",
    "        \n",
    "        for waste_type, keywords in waste_keywords.items():\n",
    "            if any(keyword in desc_lower for keyword in keywords):\n",
    "                return waste_type\n",
    "        \n",
    "        return 'general'\n",
    "\n",
    "print(\"Predictor class defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Models and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize and save model configuration\n",
    "model = MultiModalPriorityClassifier(num_priority_classes=4).to(device)\n",
    "\n",
    "# Save initial model (in production, this would be trained)\n",
    "torch.save(model.state_dict(), 'models/waste_priority_model.pth')\n",
    "print(\"Model saved to models/waste_priority_model.pth\")\n",
    "\n",
    "# Save model configuration\n",
    "config = {\n",
    "    'priority_classes': priority_classes,\n",
    "    'priority_colors': priority_colors,\n",
    "    'model_type': 'MultiModalPriorityClassifier',\n",
    "    'image_model': 'efficientnet_b0',\n",
    "    'text_model': 'distilbert-base-uncased',\n",
    "    'input_size': (224, 224),\n",
    "    'max_text_length': 128\n",
    "}\n",
    "\n",
    "with open('models/model_config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved to models/model_config.json\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test the Prediction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize predictor\n",
    "predictor = WastePriorityPredictor('models/waste_priority_model.pth')\n",
    "\n",
    "# Test with sample descriptions (without actual images)\n",
    "test_cases = [\n",
    "    {\n",
    "        'description': 'Medical waste including syringes found near school playground',\n",
    "        'expected_priority': 'Critical'\n",
    "    },\n",
    "    {\n",
    "        'description': 'Large pile of plastic waste blocking drainage system',\n",
    "        'expected_priority': 'Urgent'\n",
    "    },\n",
    "    {\n",
    "        'description': 'Scattered bottles and cans in public park',\n",
    "        'expected_priority': 'Important'\n",
    "    },\n",
    "    {\n",
    "        'description': 'Single cardboard box on sidewalk',\n",
    "        'expected_priority': 'Secondary'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING PRIORITY CLASSIFICATION SYSTEM\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    # Use rule-based prediction since we don't have actual images\n",
    "    priority, confidence = predictor.rule_based_priority(test_case['description'])\n",
    "    waste_type = predictor.detect_waste_type(test_case['description'])\n",
    "    solutions = predictor.solution_generator.generate_solution(\n",
    "        priority, waste_type, test_case['description']\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Case {i}:\")\n",
    "    print(f\"Description: {test_case['description']}\")\n",
    "    print(f\"Expected Priority: {test_case['expected_priority']}\")\n",
    "    print(f\"Predicted Priority: {priority} (Confidence: {confidence:.2%})\")\n",
    "    print(f\"Color Code: {predictor.priority_colors[priority]}\")\n",
    "    print(f\"Waste Type: {waste_type}\")\n",
    "    print(f\"\\nSuggested Solutions:\")\n",
    "    for j, solution in enumerate(solutions, 1):\n",
    "        print(f\"  {j}. {solution}\")\n",
    "    print(f\"Status: Pending\")\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Export for Production Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save predictor class and solution generator as pickle for easy loading\n",
    "import pickle\n",
    "\n",
    "# Save solution generator\n",
    "with open('models/solution_generator.pkl', 'wb') as f:\n",
    "    pickle.dump(solution_generator, f)\n",
    "\n",
    "print(\"âœ… Models exported successfully!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"1. models/waste_priority_model.pth - Main PyTorch model\")\n",
    "print(\"2. models/model_config.json - Model configuration\")\n",
    "print(\"3. models/solution_generator.pkl - Solution generation system\")\n",
    "print(\"\\nThese files can be integrated into your Flask app.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Model Performance Visualization (After Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Placeholder for training history visualization\n",
    "# This would be populated after actual training\n",
    "\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "    axes[0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Training and Validation Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    axes[1].plot(history['val_acc'], label='Validation Accuracy', color='green')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy (%)')\n",
    "    axes[1].set_title('Validation Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('models/training_history.png')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualization functions ready\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What This Notebook Does:\n",
    "\n",
    "1. **Multi-Modal Input Processing**:\n",
    "   - Image analysis using EfficientNet (pretrained on ImageNet)\n",
    "   - Text analysis using DistilBERT\n",
    "   - Combined feature fusion for priority classification\n",
    "\n",
    "2. **Priority Classification**:\n",
    "   - **Critical (Red)**: Hazardous, medical, toxic waste requiring immediate action\n",
    "   - **Urgent (Orange)**: Large dumps, blockages, environmental threats\n",
    "   - **Important (Black)**: Public space waste, recyclables\n",
    "   - **Secondary (Blue)**: Minor littering, single items\n",
    "\n",
    "3. **Solution Generation**:\n",
    "   - Context-aware suggestions based on waste type and priority\n",
    "   - Specific actions for different scenarios\n",
    "   - Integration with local resources\n",
    "\n",
    "4. **Automatic Status Management**:\n",
    "   - New reports â†’ Analyzed by model\n",
    "   - Priority assigned â†’ Status: \"Pending\"\n",
    "   - Action taken â†’ Status: \"In Progress\"\n",
    "   - Resolved â†’ Removed from active list\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Train the model on real waste images (download dataset as indicated)\n",
    "2. Fine-tune on Kenya-specific waste patterns\n",
    "3. Integrate with Flask app using the provided predictor class\n",
    "4. Deploy and monitor performance\n",
    "5. Continuously improve with real-world data\n",
    "\n",
    "### Files Generated:\n",
    "- `waste_priority_model.pth`: Trained PyTorch model\n",
    "- `model_config.json`: Model configuration\n",
    "- `solution_generator.pkl`: Solution generation system\n",
    "\n",
    "Ready for integration into Taka-Hero app! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
